{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection and Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:18:46.754605Z",
     "start_time": "2017-11-28T20:18:46.738579Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster, FastMarkerCluster\n",
    "from folium import IFrame\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import shapely.wkt\n",
    "\n",
    "from geopy.distance import vincenty, great_circle\n",
    "\n",
    "\n",
    "import re\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will extract and clean the data from the two datasets we use.\n",
    "Those dataset are the ged171 dataset from the ucdp(http://ucdp.uu.se/downloads/) and the refugee dataset from the UNHCR (http://popstats.unhcr.org/en/persons_of_concern).\n",
    "\n",
    "We will keep as much data as possible in this section, so we will avoid removing rows from our dataset.\n",
    "\n",
    "In later sections we might use subset of the full data in certain analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction and cleanup of the conflict data (ged171)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:18:48.105173Z",
     "start_time": "2017-11-28T20:18:47.064177Z"
    }
   },
   "outputs": [],
   "source": [
    "CONFLICT_DATA_PATH = \"ged171.csv\"\n",
    "raw_conflict_df = pd.read_csv(CONFLICT_DATA_PATH)\n",
    "display(raw_conflict_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:19:02.638302Z",
     "start_time": "2017-11-28T20:18:48.106787Z"
    }
   },
   "outputs": [],
   "source": [
    "for year in tqdm(raw_conflict_df.year.unique()):\n",
    "    for conflict_new_id in raw_conflict_df[raw_conflict_df.year == year].conflict_new_id.unique():\n",
    "        subdf = raw_conflict_df[(raw_conflict_df.year == year) & (raw_conflict_df.year == year)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:19:02.643156Z",
     "start_time": "2017-11-28T20:19:02.639862Z"
    }
   },
   "outputs": [],
   "source": [
    "print(raw_conflict_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:19:02.651560Z",
     "start_time": "2017-11-28T20:19:02.644797Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We kept the columns that we found meaningfull from the dataset\n",
    "# To guide our choice we used the ged171.pdf document that can be found in the repository\n",
    "KEPT_COLUMNS= ['id', 'year', 'type_of_violence', 'conflict_new_id',\n",
    "       'conflict_name', 'side_a_new_id', 'gwnoa',\n",
    "       'side_a', 'gwnob', 'side_b_new_id','side_b',\n",
    "       'where_prec', 'where_coordinates', 'adm_1', 'adm_2',\n",
    "       'latitude', 'longitude', 'geom_wkt','country',\n",
    "       'country_id','date_start', 'deaths_a', 'deaths_b',\n",
    "       'deaths_civilians',\n",
    "       'deaths_unknown', 'best'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:19:02.672017Z",
     "start_time": "2017-11-28T20:19:02.652892Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conflict_df = raw_conflict_df[KEPT_COLUMNS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:19:03.225533Z",
     "start_time": "2017-11-28T20:19:02.673737Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conflict_df = conflict_df.replace(to_replace=\"-[0-9]{2}-[0-9]{2}\", value=\"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:19:03.249850Z",
     "start_time": "2017-11-28T20:19:03.227233Z"
    }
   },
   "outputs": [],
   "source": [
    "display(conflict_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:19:03.415143Z",
     "start_time": "2017-11-28T20:19:03.251804Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('conflict.pickle', 'wb') as out:\n",
    "    pickle.dump(conflict_df, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction and cleanup of the refugee data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:19:03.506677Z",
     "start_time": "2017-11-28T20:19:03.416642Z"
    }
   },
   "outputs": [],
   "source": [
    "REFUGEE_DATA_PATH = \"unhcr_refugee.csv\"\n",
    "RAW_COLUMN_NAMES = [\"year\", \"country_dest\", \"origin\", \"refugee\",\n",
    "                \"asylum\", \"returned_refugee\", \"internally_displaced\", \"returned_idp\",\n",
    "                \"stateless\", \"others\", \"total\"\n",
    "               ]\n",
    "RAW_COLUMN_TYPE = {\"year\": int, \"coutry_dest\" : object, \"origin\" : object, \"refugee\" : float,\n",
    "               \"asylum\" : float, \"returned_refugee\" : float, \"idp\" : float, \"returned_idp\" : float,\n",
    "               \"stateless\" : float, \"others\" : float, \"total\" : float\n",
    "              }\n",
    "raw_refugee_df = pd.read_csv(REFUGEE_DATA_PATH, skiprows=4, names=RAW_COLUMN_NAMES, dtype=RAW_COLUMN_TYPE, na_values=[\"*\"])\n",
    "raw_refugee_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:19:03.534855Z",
     "start_time": "2017-11-28T20:19:03.508188Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_refugee_df.fillna(value=0, inplace=True)\n",
    "raw_refugee_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:19:03.664748Z",
     "start_time": "2017-11-28T20:19:03.536424Z"
    }
   },
   "outputs": [],
   "source": [
    "# We drop all the origins that are 'Various/Unknown', we are interested in the country of origins, so\n",
    "# this identifient is useless to our analysis\n",
    "raw_refugee_df = raw_refugee_df[(raw_refugee_df.origin != 'Various/Unknown') & (raw_refugee_df.origin != 'Stateless')]\n",
    "\n",
    "# We also drop the returned columns because it is symptomatic of past refugee and doesn't really fit in our analysis\n",
    "try:\n",
    "    raw_refugee_df.drop(['returned_refugee', 'returned_idp'], axis=1, inplace=True)\n",
    "except: # avoid error if we re-run this code\n",
    "    pass\n",
    "\n",
    "display(raw_refugee_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:19:59.988062Z",
     "start_time": "2017-11-28T20:19:03.666267Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "REFUGEE_COLUMNS = [\"year\", \"origin\", \"refugee\", \"asylum\", \"internally_displaced\", \"stateless\", \"others\", \"total\"]\n",
    "refugee_df = pd.DataFrame(columns=REFUGEE_COLUMNS)\n",
    "\n",
    "for year in tqdm(raw_refugee_df.year.unique()):\n",
    "    for origin in raw_refugee_df[raw_refugee_df.year == year].origin.unique():\n",
    "        index = (raw_refugee_df.year == year) & (raw_refugee_df.origin == origin)\n",
    "        temp_df_no_dest = raw_refugee_df[index].drop([\"country_dest\"], axis=1)\n",
    "        sum_series = temp_df_no_dest.sum(numeric_only=True)\n",
    "        # drop the row if the column of interest are zero (except total, because it might take into account returned)\n",
    "        if (sum_series[1:5] == 0).all():\n",
    "            continue\n",
    "            \n",
    "        sum_series[\"year\"] = year\n",
    "        sum_series[\"origin\"] = origin\n",
    "        sum_series[\"total\"] = sum_series[1:5].sum()\n",
    "        \n",
    "        refugee_df = refugee_df.append(sum_series, ignore_index=True)\n",
    "        \n",
    "display(refugee_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:19:59.993411Z",
     "start_time": "2017-11-28T20:19:59.989662Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('refugee.pickle', 'wb') as out:\n",
    "    pickle.dump(refugee_df, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of Networks from the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Networks are a great way to analyze the information. They can group data in a manner that can be easily visualized.\n",
    "Also, network theory can be used on them, which can extract meaningful information on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:20:00.064100Z",
     "start_time": "2017-11-28T20:19:59.994812Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('conflict.pickle', 'rb') as data_source:\n",
    "    conflict_df = pickle.load(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:20:00.070383Z",
     "start_time": "2017-11-28T20:20:00.065934Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('refugee.pickle', 'rb') as data_source:\n",
    "    refugee_df = pickle.load(data_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a network of distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split this network in two types of node: events and conflicts.\n",
    "Each conflict location will be the centroid of all its events (mean longitude and latitude)\n",
    "Each event will be linked to it's respective conflict with the edge representing the distance to the center of the conflict.\n",
    "Each conflicts will be linked together by an edge describing the distance.\n",
    "\n",
    "For each conflict node, we will also extract the statistics of its event distances with stats.describe.\n",
    "\n",
    "This network will be used to find spatial relationship between various conflicts.\n",
    "Also, because each node has a longitude and latitude information, it will be possible to print it on a map and\n",
    "selectively print links between event and conflicts or between conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:20:00.110758Z",
     "start_time": "2017-11-28T20:20:00.072221Z"
    }
   },
   "outputs": [],
   "source": [
    "display(conflict_df.head(1))\n",
    "display(refugee_df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:20:00.142488Z",
     "start_time": "2017-11-28T20:20:00.112082Z"
    }
   },
   "outputs": [],
   "source": [
    "# graph indexed\n",
    "conflict_df = conflict_df.set_index('id')\n",
    "display(conflict_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:20:00.161307Z",
     "start_time": "2017-11-28T20:20:00.144271Z"
    },
    "collapsed": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def distance_between_nodes(graph, node_1_id, node_2_id):\n",
    "    \"\"\"Get distance between two nodes by using their latitude and longitude property\"\"\"\n",
    "    pos_1 = (graph.node[node_1_id][\"latitude\"], graph.node[node_1_id][\"longitude\"])\n",
    "    pos_2 = (graph.node[node_2_id][\"latitude\"], graph.node[node_2_id][\"longitude\"])\n",
    "    # Sometime vincenty doesn't converge, just put None as weight\n",
    "    try:\n",
    "        distance = vincenty(pos_1, pos_2).km\n",
    "    except:\n",
    "        # Try to get great circle distance instead\n",
    "        try:\n",
    "            distance = great_circle(pos_1, pos_2).km\n",
    "        except:\n",
    "            print(\"Error: failed to get distance between node {} and node {}\".format(node_1_id, node_2_id))\n",
    "            print(\"Node 1 positions: {}\".format(pos_1))\n",
    "            print(\"Node 2 positions: {}\".format(pos_2))\n",
    "            distance = None\n",
    "    return distance\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:20:00.178482Z",
     "start_time": "2017-11-28T20:20:00.162846Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_conflict_lat_long(conflict_id):\n",
    "    \"\"\"Get the average latitude and longitude for a particular conflict id\"\"\"\n",
    "    longitude = conflict_df[conflict_df.conflict_new_id == conflict_id].longitude.mean()\n",
    "    latitude = conflict_df[conflict_df.conflict_new_id == conflict_id].latitude.mean()\n",
    "    return (latitude, longitude)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:20:03.792981Z",
     "start_time": "2017-11-28T20:20:00.180103Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distance_graph = nx.Graph()\n",
    "\n",
    "# Create a node for each conflict event\n",
    "distance_graph.add_nodes_from(conflict_df.index.values, nature=\"event\", year=conflict_df.year)\n",
    "\n",
    "# Set longitude and latitude for each node\n",
    "for index in conflict_df.index.values:\n",
    "    distance_graph.node[index][\"longitude\"] = conflict_df.loc[index, \"longitude\"]\n",
    "    distance_graph.node[index][\"latitude\"] = conflict_df.loc[index, \"latitude\"]\n",
    "\n",
    "# Create a node for each unique conflict, use a special node id to avoid conflict with events\n",
    "for conflict_id in conflict_df.conflict_new_id.unique():\n",
    "    conflict_node_name = \"conflict_{}\".format(conflict_id)\n",
    "    distance_graph.add_node(conflict_node_name, nature=\"conflict\")\n",
    "    \n",
    "    # Get the average position for the conflict\n",
    "    latitude, longitude = get_conflict_lat_long(conflict_id)\n",
    "    distance_graph.node[conflict_node_name][\"longitude\"] = longitude\n",
    "    distance_graph.node[conflict_node_name][\"latitude\"] = latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:20:08.070220Z",
     "start_time": "2017-11-28T20:20:03.794759Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create edges from event to their conflict\n",
    "for conflict_id in tqdm(conflict_df.conflict_new_id.unique()):\n",
    "    conflict_node_name = \"conflict_{}\".format(conflict_id)\n",
    "    \n",
    "    for event_id in conflict_df[conflict_df.conflict_new_id == conflict_id].index.values:\n",
    "        distance = distance_between_nodes(distance_graph, conflict_node_name, event_id)\n",
    "        distance_graph.add_edge(conflict_node_name, event_id, weight=distance, nature=\"event_to_conflict\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:20:38.658209Z",
     "start_time": "2017-11-28T20:20:08.071809Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the distance between all conflicts \n",
    "for conflict_id_1 in tqdm(conflict_df.conflict_new_id.unique()):\n",
    "    conflict_node_name_1 = \"conflict_{}\".format(conflict_id_1)\n",
    "    \n",
    "    for conflict_id_2 in conflict_df.conflict_new_id.unique():   \n",
    "        conflict_node_name_2 = \"conflict_{}\".format(conflict_id_2)   \n",
    "        \n",
    "        # No self loop\n",
    "        if conflict_id_1 == conflict_id_2:\n",
    "            pass\n",
    "        \n",
    "        # Don't do two times the same edge\n",
    "        if distance_graph.has_edge(conflict_node_name_1, conflict_node_name_2):\n",
    "            pass\n",
    "        \n",
    "        distance_graph.add_edge(conflict_node_name_1, conflict_node_name_2, \n",
    "                  weight = distance_between_nodes(distance_graph, conflict_node_name_1, conflict_node_name_2),\n",
    "                  nature = \"conflict_to_conflict\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:20:38.670055Z",
     "start_time": "2017-11-28T20:20:38.660081Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_event_weights(graph, node, nature=\"event_to_conflict\"):\n",
    "    \"\"\"Return the event weights for the given nature. Can be an array of natures\"\"\"\n",
    "    edges = [e[2] for e in distance_graph.edges_iter(nbunch=node, data=True)] \n",
    "    if type(nature) == str:\n",
    "        return [d[\"weight\"] for d in edges if d[\"nature\"] == nature]\n",
    "    else:\n",
    "        weights = [[] for x in range(len(nature))]\n",
    "        for index, item in enumerate(nature):\n",
    "            weights[index] = [d[\"weight\"] for d in edges if d[\"nature\"] == item]\n",
    "        return weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:20:39.649836Z",
     "start_time": "2017-11-28T20:20:38.671349Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract conflict statistics\n",
    "for node in tqdm(distance_graph.nodes()):\n",
    "    if distance_graph.node[node][\"nature\"] == \"event\":\n",
    "        continue\n",
    "    \n",
    "    weights = get_event_weights(distance_graph, node, nature=[\"event_to_conflict\", \"conflict_to_conflict\"])\n",
    "    \n",
    "    distance_graph.node[node][\"events_stats\"] = stats.describe(weights[0])\n",
    "    #distance_graph.node[node][\"conflicts_stats\"] = stats.describe(weights[1])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:20:41.758127Z",
     "start_time": "2017-11-28T20:20:39.651424Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('distance_nx.pickle', 'wb') as out:\n",
    "    pickle.dump(distance_graph, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:20:42.776840Z",
     "start_time": "2017-11-28T20:20:41.760133Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('distance_nx.pickle', 'rb') as data_source:\n",
    "    distance_graph = pickle.load(data_source) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:13:20.338860Z",
     "start_time": "2017-11-28T20:13:20.335995Z"
    }
   },
   "source": [
    "## Creation of specific Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we create dataframe from our base dataset that combine or merge important information from the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined displacement and event dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: THIS SECTION IS NOT COMPLETE YET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this section is to create a dataframe that links the displacement information given by the UNHCR with the events and deaths given by the GED dataset. This will be grouped by year and will keep track of the number of events and of the event identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:20:42.842568Z",
     "start_time": "2017-11-28T20:20:42.778440Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('conflict.pickle', 'rb') as data_source:\n",
    "    conflict_df = pickle.load(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:20:42.847419Z",
     "start_time": "2017-11-28T20:20:42.843882Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('refugee.pickle', 'rb') as data_source:\n",
    "    refugee_df = pickle.load(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:20:42.885043Z",
     "start_time": "2017-11-28T20:20:42.848756Z"
    }
   },
   "outputs": [],
   "source": [
    "display(conflict_df.head(2))\n",
    "display(refugee_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:20:42.901570Z",
     "start_time": "2017-11-28T20:20:42.886712Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_gnwo_countries_to_df():\n",
    "    \"\"\"Extract the countries from the gnwo and their id from the gnwo.txt file\"\"\"\n",
    "    with open(\"gnwo.txt\", \"r\") as gnow:\n",
    "        countries_list = []\n",
    "        for line in gnow:\n",
    "            split_line = re.split(r'\\t+', line)\n",
    "            countries_list.append(split_line[0:3])\n",
    "        # Create a dataframe\n",
    "        return pd.DataFrame(countries_list, columns=[\"id\", \"code\" ,\"name\"])\n",
    "countries = extract_gnwo_countries_to_df()\n",
    "display(countries.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:20:42.910027Z",
     "start_time": "2017-11-28T20:20:42.902968Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_standard_country_name(country_name, country_df):\n",
    "    \"\"\"Check if the country name is in the standard countries dataset\"\"\"\n",
    "    found = False\n",
    "    for country in countries.name:\n",
    "        if country_name == country:\n",
    "            found = True\n",
    "            break\n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:20:42.934469Z",
     "start_time": "2017-11-28T20:20:42.911314Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exist_in_substring(country_name, country_df):\n",
    "    \"\"\"Returns the id of the country if the substring is in the country_df names, else return none\"\"\"\n",
    "    for data in country_df.itertuples():\n",
    "        # Force ascii\n",
    "        country_name = unidecode.unidecode(country_name)\n",
    "        # manage Dem. Rep. of problem\n",
    "        country_name = re.sub(r\"Dem\\. Rep\\. of\", \"\", country_name) \n",
    "        # manage Rep. of. problem\n",
    "        country_name = re.sub(r\"Rep\\. of\", \"\", country_name) \n",
    "        # Manage diminutive problem\n",
    "        country_name = re.sub(r\"Dem\\.\", \"Democratic\", country_name)\n",
    "        country_name = re.sub(r\"Rep\\.\", \"Republic\", country_name)\n",
    "        # Manage China\n",
    "        country_name = re.sub(r\".*China.*\", \"China\", country_name)\n",
    "        # Manage Russia\n",
    "        country_name = re.sub(r\"Russian Federation\", \"Russia\", country_name)\n",
    "\n",
    "\n",
    "        try:\n",
    "            if country_name in data.name:\n",
    "                return data\n",
    "            elif country_name.split()[0] in data.name:\n",
    "                return data\n",
    "        except:\n",
    "            print(\"EXCEPTION!!!\")\n",
    "            print(country_name, data)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:20:43.101375Z",
     "start_time": "2017-11-28T20:20:42.936033Z"
    }
   },
   "outputs": [],
   "source": [
    "for country in refugee_df.origin.unique():\n",
    "    found = is_standard_country_name(country, countries)\n",
    "    \n",
    "    # We constructed a dictionnary manually to change the names\n",
    "    if not found:\n",
    "        if not exist_in_substring(country, countries):\n",
    "            print(country)\n",
    "        #print(country)\n",
    "        #print(change_country_names_refugee_dict[country])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:24:14.298538Z",
     "start_time": "2017-11-28T20:24:14.293729Z"
    }
   },
   "source": [
    "In this section we provide sample visualization of the datasets.\n",
    "For now they are mainly tests to validate the capacities of the respective libraries with datasets of the size we use.\n",
    "\n",
    "Obviously, they will be refined during the last segment of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Examples of visualisation on a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:20:43.166945Z",
     "start_time": "2017-11-28T20:20:43.102822Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('conflict.pickle', 'rb') as data_source:\n",
    "    conflict_df = pickle.load(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:20:45.024063Z",
     "start_time": "2017-11-28T20:20:43.168433Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "geometry = conflict_df['geom_wkt'].map(shapely.wkt.loads)\n",
    "conflict_df = conflict_df.drop('geom_wkt', axis=1)\n",
    "crs = {'init': 'epsg:4326'}\n",
    "gdf = gpd.GeoDataFrame(conflict_df, crs=crs, geometry=geometry)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:20:52.515730Z",
     "start_time": "2017-11-28T20:20:45.025816Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "\n",
    "\n",
    "world.plot(ax=ax, color='white', edgecolor='black')\n",
    "gdf.plot(ax=ax, markersize=(gdf.loc[:, \"best\"]/1000), cmap=\"autumn\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display using folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:20:52.548433Z",
     "start_time": "2017-11-28T20:20:52.517347Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = folium.Map(tiles='cartodbpositron')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-28T20:20:52.607273Z",
     "start_time": "2017-11-28T20:20:52.549946Z"
    },
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "geojson_gdf = gdf[:50].to_crs(epsg='4326').to_json()\n",
    "points = folium.features.GeoJson(geojson_gdf)\n",
    "m.add_child(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We attemted and succeeded to plot all the events on a map.\n",
    "\n",
    "The code used is **VERY** slow on the notebook because of the size of the dataset.\n",
    "\n",
    "Because of this, we used a separate python script to run it ( see \"fast_map.py\")\n",
    "\n",
    "The resulting map is named \"output_map.html\" and can be found in the git folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
