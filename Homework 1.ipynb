{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1\"><a href=\"#Task-1.-Compiling-Ebola-Data\"><span class=\"toc-item-num\">Task 1.&nbsp;&nbsp;</span>Compiling Ebola Data</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-2.-RNA-Sequences\"><span class=\"toc-item-num\">Task 2.&nbsp;&nbsp;</span>RNA Sequences</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-3.-Class-War-in-Titanic\"><span class=\"toc-item-num\">Task 3.&nbsp;&nbsp;</span>Class War in Titanic</a></div></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"/home/vinz/Desktop/ADA/ADA2017-Tutorials/02 - Intro to Pandas/Data\" # Use the data folder provided in Tutorial 02 - Intro to Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Compiling Ebola Data\n",
    "\n",
    "The `DATA_FOLDER/ebola` folder contains summarized reports of Ebola cases from three countries (Guinea, Liberia and Sierra Leone) during the recent outbreak of the disease in West Africa. For each country, there are daily reports that contain various information about the outbreak in several cities in each country.\n",
    "\n",
    "Use pandas to import these data files into a single `Dataframe`.\n",
    "Using this `DataFrame`, calculate for *each country*, the *daily average* per year of *new cases* and *deaths*.\n",
    "Make sure you handle all the different expressions for *new cases* and *deaths* that are used in the reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy\n",
    "\n",
    "We will import and wrangle the data for each country separately and then combine everything into a single data frame.\n",
    "\n",
    "As the data files already contain variables with nation wide statistics, we will use these values instead of manually aggregating the data city by city."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As specified on Mattermost, we track for both new cases and deaths the probable, suspected and confirmed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ebola_csv_iter(folder):\n",
    "    \"\"\"Utility function returning an iterator over the csv files in given folder\"\"\"\n",
    "    path = ''.join([DATA_FOLDER, '/ebola/', folder, '/'])\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".csv\"):\n",
    "            yield file, ''.join([path, file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def associate_to_country(country, df):\n",
    "    \"\"\"Adds a country super index to the provided dataframe\"\"\"\n",
    "    country = pd.DataFrame({\n",
    "        'Country': np.repeat(country, len(df)),\n",
    "        'Date': df.index\n",
    "    })\n",
    "    return country.merge(df, right_index=True, left_on='Date').set_index(['Country', 'Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_num(x):\n",
    "    try:\n",
    "        return np.float32(x)\n",
    "    except ValueError:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guinea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "tables_with_duplicates = []\n",
    "interesting_fields = {\n",
    "    #'New deaths registered today (confirmed)': 'Death confirmed',\n",
    "    #'New deaths registered today (probables)': 'Death probable',\n",
    "    #'New deaths registered today (suspects)': 'Death suspected',\n",
    "    'Total deaths of suspects': 'Death suspected',\n",
    "    'Total deaths of probables': 'Death probable',\n",
    "    'Total deaths of confirmed': 'Death confirmed',\n",
    "    'New cases of confirmed': 'New cases confirmed',\n",
    "    'New cases of probables': 'New cases probable', \n",
    "    'New cases of suspects': 'New cases suspected'\n",
    "}\n",
    "for file, path in ebola_csv_iter('guinea_data'):\n",
    "    df = pd.read_csv(path,\n",
    "                     parse_dates=['Date'],\n",
    "                     usecols=['Date', 'Description', 'Totals'],\n",
    "                     converters={'Totals': parse_num})\n",
    "    df.rename(columns={'Description': 'Variable', 'Totals': 'National'}, inplace=True)\n",
    "    # Check for duplicates\n",
    "    df = df[df.Variable.isin(interesting_fields)]\n",
    "    if len(df.Variable.unique()) != len(df.Variable):\n",
    "        tables_with_duplicates.append(file)\n",
    "    dfs[file] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_with_duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are no duplicates, we can pivot all the dataframes to have a timeseries dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfs = [df.pivot(index='Date', columns='Variable', values='National') for df in dfs.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "guinea = pd.concat(dfs).sort_index() # Create the data series: index=date\n",
    "guinea = associate_to_country('Guinea', guinea).rename(columns=interesting_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started by using the fields:\n",
    "\n",
    " - `New deaths registered today (confirmed)`\n",
    " - `New deaths registered today (probables)`\n",
    " - `New deaths registered today (suspects)`\n",
    " \n",
    "but they were empty for all days except one. Therefore we decided to use the cumulated values and perform the subtraction to obtain daily counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deaths = guinea[['Death confirmed', 'Death probable', 'Death suspected']]\n",
    "cleaned = deaths - deaths.shift(1) # row_i - row_{i - 1}\n",
    "guinea.loc[:, ['Death confirmed', 'Death probable', 'Death suspected']] = cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation relies on a very important assumption. \n",
    "We repeat the same computation on the data concerning Liberia and Sierra Leone.\n",
    "We assume that, given that the counts are cumulative, if we are missing a value for day $j$ and we have a value for day $i > j$, the value for day $j$ is counted in day $i$. **In other words, even if data for some days is missing, the totals are correct.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "guinea.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liberia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "tables_with_duplicates = []\n",
    "interesting_fields = {\n",
    "    'Total death/s in confirmed cases': 'Death confirmed',\n",
    "    'Total death/s in probable cases': 'Death probable',\n",
    "    'Total death/s in suspected cases': 'Death suspected',\n",
    "    'New case/s (confirmed)': 'New cases confirmed',\n",
    "    'New Case/s (Probable)': 'New cases probable',\n",
    "    'New Case/s (Suspected)': 'New cases suspected'\n",
    "}\n",
    "for file, path in ebola_csv_iter('liberia_data'):\n",
    "    df = pd.read_csv(path,\n",
    "                     parse_dates=['Date'],\n",
    "                     usecols=['Date', 'Variable', 'National'],\n",
    "                     converters={'National': parse_num})\n",
    "    df = df[df.Variable.isin(interesting_fields)]\n",
    "    # Check for duplicates\n",
    "    if len(df.Variable.unique()) != len(df.Variable):\n",
    "        tables_with_duplicates.append(file)\n",
    "    dfs[file] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remark that only a data file contains duplicate variables for the same day. Before dropping the duplicates, we look at the duplicated lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_with_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with_duplicates = dfs[tables_with_duplicates[0]]\n",
    "with_duplicates[with_duplicates.duplicated(keep=False, subset='Variable')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 Variables are repeated twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first = with_duplicates[with_duplicates.duplicated(keep='last', subset='Variable')][['Variable', 'National']]\n",
    "second = with_duplicates[with_duplicates.duplicated(keep='first', subset='Variable')][['Variable', 'National']]\n",
    "first = first.set_index('Variable')\n",
    "second = second.set_index('Variable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second - first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to keep the bigger values as the difference is not very relevant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with_duplicates.drop_duplicates(subset='Variable', keep='last', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfs = [df.pivot(index='Date', columns='Variable', values='National') for df in dfs.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "liberia = pd.concat(dfs).sort_index()\n",
    "liberia = associate_to_country('Liberia', liberia).rename(columns=interesting_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "liberia.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However in this dataframe, the three deaths columns are *cumulative*. We can therefore subtract the value from the previous line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deaths = liberia[['Death confirmed', 'Death probable', 'Death suspected']]\n",
    "cleaned = deaths - deaths.shift(1) # row_i - row_{i - 1}\n",
    "liberia.loc[:, ['Death confirmed', 'Death probable', 'Death suspected']] = cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_scale = liberia[-6:][['New cases probable', 'New cases suspected', 'New cases confirmed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "liberia.loc[-6:, ['New cases probable', 'New cases suspected', 'New cases confirmed']] = to_scale - to_scale.shift(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sierra Leone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "tables_with_duplicates = []\n",
    "interesting_fields = {\n",
    "    'death_confirmed': 'Death confirmed',\n",
    "    'death_probable': 'Death probable',\n",
    "    'death_suspected': 'Death suspected',\n",
    "    'new_confirmed': 'New cases confirmed',\n",
    "    'new_probable': 'New cases probable',\n",
    "    'new_suspected': 'New cases suspected'\n",
    "}\n",
    "\n",
    "for file, path in ebola_csv_iter('sl_data'):\n",
    "    df = pd.read_csv(path,\n",
    "                     parse_dates=['date'],\n",
    "                     usecols=['date', 'variable', 'National'], \n",
    "                     converters={'National': parse_num})\n",
    "    df.rename(columns={'date': 'Date', 'variable': 'Variable'}, inplace=True)\n",
    "    df = df[df.Variable.isin(interesting_fields)]\n",
    "    if len(df.Variable.unique()) != len(df.Variable):\n",
    "        tables_with_duplicates.append(file)\n",
    "    dfs[file] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_with_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfs = [df.pivot(index='Date', columns='Variable', values='National') for df in dfs.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sl = pd.concat(dfs).sort_index()\n",
    "sl = associate_to_country('Sierra Leone', sl).rename(columns=interesting_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sl.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fields concerning the number of deaths are cumulative. We apply the same treatment as in the previous cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deaths = sl[['Death confirmed', 'Death probable', 'Death suspected']]\n",
    "cleaned = deaths - deaths.shift(1) # row_i - row_{i - 1}\n",
    "sl.loc[:, ['Death confirmed', 'Death probable', 'Death suspected']] = cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del dfs\n",
    "del tables_with_duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate all dataframes\n",
    "df = pd.concat([guinea, liberia, sl])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def month_average(group):\n",
    "    # Only divide by the number of days counted in the set.\n",
    "    registered_days = group.index.get_level_values(1).max().day\n",
    "    return group.sum() / registered_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import calendar\n",
    "# Compute mean per month per country. \n",
    "results = df.groupby(lambda row: (row[0], row[1].month))\\\n",
    "            .agg(month_average)\n",
    "\n",
    "# Give computation result a nice index\n",
    "results.index = pd.MultiIndex.from_tuples(results.index, names=['Country', 'Month'])\\\n",
    "                  .map(lambda i: (i[0], calendar.month_name[i[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. RNA Sequences\n",
    "\n",
    "In the `DATA_FOLDER/microbiome` subdirectory, there are 9 spreadsheets of microbiome data that was acquired from high-throughput RNA sequencing procedures, along with a 10<sup>th</sup> file that describes the content of each. \n",
    "\n",
    "Use pandas to import the first 9 spreadsheets into a single `DataFrame`.\n",
    "Then, add the metadata information from the 10<sup>th</sup> spreadsheet as columns in the combined `DataFrame`.\n",
    "Make sure that the final `DataFrame` has a unique index and all the `NaN` values have been replaced by the tag `unknown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup for location of the dataset of the task 2\n",
    "MICROBIOME_FOLDER = DATA_FOLDER + \"/microbiome\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Basic analysis of the file formatting:\n",
    "\n",
    "***\n",
    "**For the files MIDn.xls with n in [1,9]**\n",
    "\n",
    "**column 1:**\n",
    "\n",
    "We see that the first column in the files contain the scientific classification of the microbiomes\n",
    "Altough it could be kept as a single string, it would have more meaning if splitted\n",
    "The scientific classification contains the following subdivisions (https://en.wikipedia.org/wiki/Taxonomic_rank): \n",
    "\n",
    "       Domain, Kingdom, Phylum, Class, Order, Family, Genus, Species\n",
    "       \n",
    " With this classification, we have a problem: only 6 strings are given in the data set, whilst we have 8 potential divisions in the classification.\n",
    " \n",
    " First, we note that in the scientific classification, Kingdom is not used with Bacterias (https://en.wikipedia.org/wiki/Bacteria) and in Archeas, it is always the same as the Phylum (https://en.wikipedia.org/wiki/Archaea). After checking the strings in the data set, it indeed seems that the Kingdom is never given. Therefore, **we will not use Kingdom as a division**.\n",
    " \n",
    " Also, Species only has a sense in the Eucaryote Domain, for which we have no data, so **we will not use Species as a division**.\n",
    " \n",
    " In addition, after playing with the data, we found cases where the Family can be named: \"Incertae Sedis\", which would be classified as the family and genus in our classification. Therefore, we need to check for those cases and re-concatenate the strings to return a proper list in every cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets keep those names in a list for further use\n",
    "scientific_classification = [\"domain\", \"phylum\", \"class\", \"order\", \"family\", \"genus\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Column 2:**\n",
    "\n",
    "We see that the second column contains an integer value. We can suppose that this value is the number of samples\n",
    "containing the genus defined by the first column. There's no title to the column, so we don't know yet where\n",
    "those values came from.\n",
    "\n",
    "***\n",
    "\n",
    "**For the file \"Metadata.xls\"**\n",
    "\n",
    "**Column 1:**\n",
    "\n",
    "Titled **\"BARCODE\"**, give xls file identifier for which the two other columns give more information\n",
    "\n",
    "**Column 2:**\n",
    "\n",
    "Titled **\"GROUP\"**, gives the groups from which each dataset has been sampled. Two informations are contained in those groups, the first is given by \"NEC\", \"Control\" or \"EXTRACTION CONTROL\" and the second is the numbering of the group (which is likely a phase of test), either \"1\" or \"2\". We will split those informations in two columns, because while using the dataset, we might want to combine all the \"NEC\" patients or all the patient for a specific phase.\n",
    "\n",
    "**Column 3:**\n",
    "\n",
    "Titled **\"SAMPLE\"**, gives the type of sample that was taken, either tissue, stool or NA. Each group had both types of samples taken. \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desired formating of the data after analysis\n",
    "\n",
    "The simple analysis above tells us what are the columns that we will want in our Data Frame\n",
    "\n",
    "1. **6** column for classification, 1 for each classifier in the \"scientific_classification\" list. This will caracterise each microbiome individually.\n",
    "\n",
    "2. **1** column will contain the value associated with each microbiome measurement.\n",
    "\n",
    "3. **2** columns describing the group of the sample from the metadata. The first column will be called **Group Type** and will contain the \"NEC\", \"Control\" or \"EXTRACTION CONTROL\" value. The second column will be called **Group Phase** and will contain either \"1\", \"2\" or \"unknow\", \"unknow\" will be used for the \"EXCTRACTON CONTROL\" group.\n",
    "\n",
    "4. **1** column describing the type of sample taken from the metadata. This column will be called **Sample** This will be either \"tissue\", \"stool\" or \"unknow\", \"unknow\" in the case of the EXTRACTION CONTROL group.\n",
    "\n",
    "For a total of **10** columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metadata_list = [\"group_type\", \"group_phase\", \"sample\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_col_list = scientific_classification + metadata_list + [\"value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_col_list))\n",
    "print(df_col_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### We want to extract the metadata from the metadata.xls file\n",
    "\n",
    "The metadata is needed before we start extracting the data from the MB files to create the DataFrame with all the desired columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The actual name of the excel sheet is \"Sheet1\" and not \"Sheet 1\" as in the other files.\n",
    "metadata_raw = pd.read_excel(MICROBIOME_FOLDER+\"/metadata.xls\", sheetname='Sheet1', header=0)\n",
    "metadata_raw.columns = metadata_raw.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Extract group phase and group type\n",
    "groups = metadata_raw[\"group\"]\n",
    "group_type = []\n",
    "group_phase = []\n",
    "for group in groups:\n",
    "    # A special case for the extraction control, we don't want to split it\n",
    "    if group == \"EXTRACTION CONTROL\":\n",
    "        group_type.append(group)\n",
    "        group_phase.append(\"\")\n",
    "    else:\n",
    "        type, phase, *_ = group.split()\n",
    "        group_type.append(type)\n",
    "        group_phase.append(phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_raw['group_type'] = pd.Series(group_type)\n",
    "metadata_raw['group_phase'] = pd.Series(group_phase)\n",
    "metadata = metadata_raw.drop(\"group\",  axis=1)\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure SAMPLE doesn't contain NaN, replace it by an empty string\n",
    "# This will give use prettier column names later on\n",
    "metadata[\"sample\"].fillna(value=\"\", inplace=True)\n",
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format of the metadata is now as we want it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### We now want to extract the data from the datasheets\n",
    "\n",
    "** We create a function to extract the classifications **\n",
    "\n",
    "We want to extract the classifications from the first column of the datasheets. We will need to do string parsing. Regex are a great tool to remove the unwanted characters from the format. The only unwanted characters are the double quotes \". Also, as said before, we need to manage the case where we have the name \"Incertae Sedis\" as a Family (4th position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_classifiers(classifier_string):\n",
    "    \"\"\"Replaces \" in the classifier_string and \n",
    "    splits the string to have an indexable list\n",
    "    \"\"\"\n",
    "    classified_list = re.subn(\"\\\"\", \"\", classifier_string)[0].lower().split()\n",
    "    \n",
    "    # Special case management\n",
    "    if len(classified_list) > 6:\n",
    "        # We join the first extra location with the family\n",
    "        classified_list[4] = ' '.join([classified_list[4], classified_list[5]])\n",
    "        del classified_list[5]\n",
    "        \n",
    "        # Allow for harbitrary number of words in the genus\n",
    "        classified_list[5] = ' '.join(classified_list[6:])\n",
    "        del classified_list[6:]\n",
    "        \n",
    "    return classified_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** We can now attempt to create the desired dataframe **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# In this section, we test the analysis of a single datasheet\n",
    "test_data = pd.read_excel(MICROBIOME_FOLDER+\"/MID1.xls\", sheetname='Sheet 1', header=None)\n",
    "test_data.columns = [\"raw_classification\", \"value\"]\n",
    "classifier_df = pd.DataFrame(columns=df_col_list)\n",
    "classifier_array = [get_classifiers(row.raw_classification) for row in test_data.itertuples()]\n",
    "classifier_series = pd.DataFrame(classifier_array, columns=scientific_classification)\n",
    "classifier_series[\"value\"] = test_data[\"value\"]\n",
    "classifier_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We reorder the metadata here, this will order the resulting data\n",
    "metadata = metadata[[\"barcode\", \"group_phase\", \"group_type\", \"sample\"]]\\\n",
    "              .sort_values(by=[ 'group_phase', 'group_type', 'sample'])\n",
    "metadata.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loop over all the datasheets\n",
    "clean_data = pd.DataFrame(columns=scientific_classification)\n",
    "for metadata_row in metadata.itertuples():\n",
    "    raw_data = pd.read_excel(MICROBIOME_FOLDER+\"/\"+metadata_row.barcode+\".xls\", sheetname='Sheet 1', header=None)\n",
    "    # Change column names to something clearer\n",
    "    raw_data.columns = [\"raw_classification\", \"value\"]\n",
    "    \n",
    "    # For each datasheet create a local classified set of data\n",
    "    classifier_array = [get_classifiers(row.raw_classification) for row in raw_data.itertuples()]\n",
    "    local_classified = pd.DataFrame(classifier_array, columns=scientific_classification)\n",
    "    \n",
    "    # Add the columns that are not the classification to the local set of data \n",
    "    local_classified[str(metadata_row.barcode)]  = test_data[\"value\"]\n",
    "    # Add the local data to the clean DataFrame\n",
    "    clean_data = pd.merge(clean_data, local_classified, how=\"outer\", on=scientific_classification)\n",
    "\n",
    "clean_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Lets find all rows that contains only NaN values **\n",
    "\n",
    "Those rows are not usefull for the analysis and we will drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a table with True in place of a row where all the MID values are null\n",
    "table_of_null_row = pd.isnull(clean_data[metadata.barcode]).all(axis=1)\n",
    "\n",
    "# Get the associated indexes\n",
    "index_of_null_row = table_of_null_row[table_of_null_row].index[:]\n",
    "\n",
    "# Lets check that the value are actually null\n",
    "clean_data.iloc[index_of_null_row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_data = clean_data.drop(index_of_null_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Now lets replace all the None values by unknow **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_data.fillna(value=\"unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clean_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that some genus are unknown... This seems a bit odd, so we will search if one other of the term is the actual genus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_genus = clean_data[clean_data.genus == 'unknown']\n",
    "unknown_genus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the list above, we can see that we have many cases where we have a string containing \"incertae_sedis\". Normally, this nomenclature is used when a genus doesn't have clear parents (https://en.wikipedia.org/wiki/Incertae_sedis). Because of this we will attempt to use the first class after the incertae_sedis as the name of the genus and keep the other values as unknown.\n",
    "\n",
    "In the case of \"(some term)_genera_incertae_sedis\", it means that the term is the genus and the rest is unknown, so we will do a special parse for those cases.\n",
    "\n",
    "In cases where we don't have incertae sedis, we will avoid doing manipulations. We don't want to induce errors in the dataset because we managed poorly the nomenclature for the specific cases. Because there's a small amount of thoses cases left after the analysis, it could be asked to an expert in the domain or obtained with more advanced research on the subject to add a case by case filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_incertae_genus(class_, order):\n",
    "    \"\"\"Extracts genus from either class or order if either of them contain incertae_sedis\"\"\"\n",
    "    for target in [class_, order]:\n",
    "        genus, *rest = target.split('_incertae_sedis')\n",
    "        if len(rest) > 0:\n",
    "            return genus\n",
    "    return 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genuses = [get_incertae_genus(c, o) for *_, c, o in unknown_genus[['class', 'order']].itertuples()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Insert newly retrieved genus names\n",
    "clean_data.loc[clean_data.genus == 'unknown', 'genus'] = genuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the remaining unknown genus, those will not be managed and would be left for later manual management\n",
    "clean_data[clean_data.genus == 'unknown']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replace the incertae sedis by unknown**\n",
    "\n",
    "In the cases where we have incertae sedis in one of the column, we will replace the value incertae_sedis by unknown. It keeps the actual information because we already setted the associated genus in the previous step. This will allow for a cleaner overall dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_all_incertae_sedis(data_frame):\n",
    "    \"\"\" This function extract all the incertae sedis from the class, order and family of a dataframe\"\"\"\n",
    "    print(\"Found incertae sedis in:\")\n",
    "    for i, c, o, f in clean_data[[\"class\", \"order\", \"family\"]].itertuples():\n",
    "        if \"_incertae_sedis\" in c or \"incertae sedis\" in c:\n",
    "            print(\"Class '\", c, \"' at index \", i)\n",
    "        elif \"_incertae_sedis\" in o or \"incertae sedis\" in o:\n",
    "            print(\"Order '\", o, \"' at index \", i)\n",
    "        elif \"_incertae_sedis\" in f or \"incertae sedis\" in f:\n",
    "            print(\"Family '\", f, \"' at index \", i)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show that there's incertae sedis in the data frame \"clean_data\"\n",
    "print_all_incertae_sedis(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the panda library to find all incertae sedis in the data set and replace it by unknown\n",
    "for column in clean_data.columns:\n",
    "    clean_data[column].replace(to_replace=\".*incertae[_ ]sedis\",\n",
    "                               value=\"unknown\",\n",
    "                               inplace=True,\n",
    "                               regex=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_all_incertae_sedis(clean_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Now lets manage the index of the DataFrame **\n",
    "\n",
    "For now, we consider that every column but the value can be considered as a metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_data = clean_data.set_index(list(scientific_classification))\n",
    "indexed_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_data.index.is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Now lets give the column to more meaningfull names **\n",
    "\n",
    "The ordering of the columns is already managed to give a nice output. This is why we reordered the metadata earlier on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_data = pd.DataFrame(data=indexed_data.values,\n",
    "                           index=indexed_data.index,\n",
    "                           columns=[metadata.group_phase.get_values(),\n",
    "                                    metadata.group_type.get_values(),\n",
    "                                    metadata[\"sample\"].get_values()])\n",
    "pretty_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Lets give a name to the columns\n",
    "pretty_data.columns.names = [\"Group Number\", \"Group Type\", \"Sample\"]\n",
    "pretty_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets export the resulting data for easy referencing\n",
    "pretty_data.to_csv(\"resulting_data.csv\")"
   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Class War in Titanic\n",
    "\n",
    "Use pandas to import the data file `Data/titanic.xls`. It contains data on all the passengers that travelled on the Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(filename=DATA_FOLDER+'/titanic.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the following questions state clearly your assumptions and discuss your findings:\n",
    "1. Describe the *type* and the *value range* of each attribute. Indicate and transform the attributes that can be `Categorical`. \n",
    "2. Plot histograms for the *travel class*, *embarkation port*, *sex* and *age* attributes. For the latter one, use *discrete decade intervals*. \n",
    "3. Calculate the proportion of passengers by *cabin floor*. Present your results in a *pie chart*.\n",
    "4. For each *travel class*, calculate the proportion of the passengers that survived. Present your results in *pie charts*.\n",
    "5. Calculate the proportion of the passengers that survived by *travel class* and *sex*. Present your results in *a single histogram*.\n",
    "6. Create 2 equally populated *age categories* and calculate survival proportions by *age category*, *travel class* and *sex*. Present your results in a `DataFrame` with unique index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
