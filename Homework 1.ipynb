{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1\"><a href=\"#Task-1.-Compiling-Ebola-Data\"><span class=\"toc-item-num\">Task 1.&nbsp;&nbsp;</span>Compiling Ebola Data</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-2.-RNA-Sequences\"><span class=\"toc-item-num\">Task 2.&nbsp;&nbsp;</span>RNA Sequences</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-3.-Class-War-in-Titanic\"><span class=\"toc-item-num\">Task 3.&nbsp;&nbsp;</span>Class War in Titanic</a></div></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"/home/vinz/Desktop/ADA/ADA2017-Tutorials/02 - Intro to Pandas/Data\" # Use the data folder provided in Tutorial 02 - Intro to Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Compiling Ebola Data\n",
    "\n",
    "The `DATA_FOLDER/ebola` folder contains summarized reports of Ebola cases from three countries (Guinea, Liberia and Sierra Leone) during the recent outbreak of the disease in West Africa. For each country, there are daily reports that contain various information about the outbreak in several cities in each country.\n",
    "\n",
    "Use pandas to import these data files into a single `Dataframe`.\n",
    "Using this `DataFrame`, calculate for *each country*, the *daily average* per year of *new cases* and *deaths*.\n",
    "Make sure you handle all the different expressions for *new cases* and *deaths* that are used in the reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy\n",
    "\n",
    "We will import and wrangle the data for each country separately and then combine everything into a single data frame.\n",
    "\n",
    "As the data files already contain variables with nation wide statistics, we will use these values instead of manually aggregating the data city by city."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As specified on Mattermost, we track for both new cases and deaths the probable, suspected and confirmed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ebola_csv_iter(folder):\n",
    "    \"\"\"Utility function returning an iterator over the csv files in given folder\"\"\"\n",
    "    path = ''.join([DATA_FOLDER, '/ebola/', folder, '/'])\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".csv\"):\n",
    "            yield file, ''.join([path, file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def associate_to_country(country, df):\n",
    "    \"\"\"Adds a country super index to the provided dataframe\"\"\"\n",
    "    country = pd.DataFrame({\n",
    "        'Country': np.repeat(country, len(df)),\n",
    "        'Date': df.index\n",
    "    })\n",
    "    return country.merge(df, right_index=True, left_on='Date').set_index(['Country', 'Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_num(x):\n",
    "    try:\n",
    "        return np.float32(x)\n",
    "    except ValueError:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guinea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "tables_with_duplicates = []\n",
    "interesting_fields = {\n",
    "    #'New deaths registered today (confirmed)': 'Death confirmed',\n",
    "    #'New deaths registered today (probables)': 'Death probable',\n",
    "    #'New deaths registered today (suspects)': 'Death suspected',\n",
    "    'Total deaths of suspects': 'Death suspected',\n",
    "    'Total deaths of probables': 'Death probable',\n",
    "    'Total deaths of confirmed': 'Death confirmed',\n",
    "    'New cases of confirmed': 'New cases confirmed',\n",
    "    'New cases of probables': 'New cases probable', \n",
    "    'New cases of suspects': 'New cases suspected'\n",
    "}\n",
    "for file, path in ebola_csv_iter('guinea_data'):\n",
    "    df = pd.read_csv(path,\n",
    "                     parse_dates=['Date'],\n",
    "                     usecols=['Date', 'Description', 'Totals'],\n",
    "                     converters={'Totals': parse_num})\n",
    "    df.rename(columns={'Description': 'Variable', 'Totals': 'National'}, inplace=True)\n",
    "    # Check for duplicates\n",
    "    df = df[df.Variable.isin(interesting_fields)]\n",
    "    if len(df.Variable.unique()) != len(df.Variable):\n",
    "        tables_with_duplicates.append(file)\n",
    "    dfs[file] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_with_duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are no duplicates, we can pivot all the dataframes to have a timeseries dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfs = [df.pivot(index='Date', columns='Variable', values='National') for df in dfs.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "guinea = pd.concat(dfs).sort_index() # Create the data series: index=date\n",
    "guinea = associate_to_country('Guinea', guinea).rename(columns=interesting_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started by using the fields:\n",
    "\n",
    " - `New deaths registered today (confirmed)`\n",
    " - `New deaths registered today (probables)`\n",
    " - `New deaths registered today (suspects)`\n",
    " \n",
    "but they were empty for all days except one. Therefore we decided to use the cumulated values and perform the subtraction to obtain daily counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deaths = guinea[['Death confirmed', 'Death probable', 'Death suspected']]\n",
    "cleaned = deaths - deaths.shift(1) # row_i - row_{i - 1}\n",
    "guinea.loc[:, ['Death confirmed', 'Death probable', 'Death suspected']] = cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation relies on a very important assumption. \n",
    "We repeat the same computation on the data concerning Liberia and Sierra Leone.\n",
    "We assume that, given that the counts are cumulative, if we are missing a value for day $j$ and we have a value for day $i > j$, the value for day $j$ is counted in day $i$. **In other words, even if data for some days is missing, the totals are correct.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "guinea.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liberia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "tables_with_duplicates = []\n",
    "interesting_fields = {\n",
    "    'Total death/s in confirmed cases': 'Death confirmed',\n",
    "    'Total death/s in probable cases': 'Death probable',\n",
    "    'Total death/s in suspected cases': 'Death suspected',\n",
    "    'New case/s (confirmed)': 'New cases confirmed',\n",
    "    'New Case/s (Probable)': 'New cases probable',\n",
    "    'New Case/s (Suspected)': 'New cases suspected'\n",
    "}\n",
    "for file, path in ebola_csv_iter('liberia_data'):\n",
    "    df = pd.read_csv(path,\n",
    "                     parse_dates=['Date'],\n",
    "                     usecols=['Date', 'Variable', 'National'],\n",
    "                     converters={'National': parse_num})\n",
    "    df = df[df.Variable.isin(interesting_fields)]\n",
    "    # Check for duplicates\n",
    "    if len(df.Variable.unique()) != len(df.Variable):\n",
    "        tables_with_duplicates.append(file)\n",
    "    dfs[file] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remark that only a data file contains duplicate variables for the same day. Before dropping the duplicates, we look at the duplicated lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_with_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with_duplicates = dfs[tables_with_duplicates[0]]\n",
    "with_duplicates[with_duplicates.duplicated(keep=False, subset='Variable')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 Variables are repeated twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first = with_duplicates[with_duplicates.duplicated(keep='last', subset='Variable')][['Variable', 'National']]\n",
    "second = with_duplicates[with_duplicates.duplicated(keep='first', subset='Variable')][['Variable', 'National']]\n",
    "first = first.set_index('Variable')\n",
    "second = second.set_index('Variable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second - first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to keep the bigger values as the difference is not very relevant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with_duplicates.drop_duplicates(subset='Variable', keep='last', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfs = [df.pivot(index='Date', columns='Variable', values='National') for df in dfs.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "liberia = pd.concat(dfs).sort_index()\n",
    "liberia = associate_to_country('Liberia', liberia).rename(columns=interesting_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "liberia.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However in this dataframe, the three deaths columns are *cumulative*. We can therefore subtract the value from the previous line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deaths = liberia[['Death confirmed', 'Death probable', 'Death suspected']]\n",
    "cleaned = deaths - deaths.shift(1) # row_i - row_{i - 1}\n",
    "liberia.loc[:, ['Death confirmed', 'Death probable', 'Death suspected']] = cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_scale = liberia[-6:][['New cases probable', 'New cases suspected', 'New cases confirmed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "liberia.loc[-6:, ['New cases probable', 'New cases suspected', 'New cases confirmed']] = to_scale - to_scale.shift(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sierra Leone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "tables_with_duplicates = []\n",
    "interesting_fields = {\n",
    "    'death_confirmed': 'Death confirmed',\n",
    "    'death_probable': 'Death probable',\n",
    "    'death_suspected': 'Death suspected',\n",
    "    'new_confirmed': 'New cases confirmed',\n",
    "    'new_probable': 'New cases probable',\n",
    "    'new_suspected': 'New cases suspected'\n",
    "}\n",
    "\n",
    "for file, path in ebola_csv_iter('sl_data'):\n",
    "    df = pd.read_csv(path,\n",
    "                     parse_dates=['date'],\n",
    "                     usecols=['date', 'variable', 'National'], \n",
    "                     converters={'National': parse_num})\n",
    "    df.rename(columns={'date': 'Date', 'variable': 'Variable'}, inplace=True)\n",
    "    df = df[df.Variable.isin(interesting_fields)]\n",
    "    if len(df.Variable.unique()) != len(df.Variable):\n",
    "        tables_with_duplicates.append(file)\n",
    "    dfs[file] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_with_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfs = [df.pivot(index='Date', columns='Variable', values='National') for df in dfs.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sl = pd.concat(dfs).sort_index()\n",
    "sl = associate_to_country('Sierra Leone', sl).rename(columns=interesting_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sl.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fields concerning the number of deaths are cumulative. We apply the same treatment as in the previous cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deaths = sl[['Death confirmed', 'Death probable', 'Death suspected']]\n",
    "cleaned = deaths - deaths.shift(1) # row_i - row_{i - 1}\n",
    "sl.loc[:, ['Death confirmed', 'Death probable', 'Death suspected']] = cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del dfs\n",
    "del tables_with_duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenate all dataframes\n",
    "df = pd.concat([guinea, liberia, sl])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def month_average(group):\n",
    "    # Only divide by the number of days counted in the set.\n",
    "    registered_days = group.index.get_level_values(1).max().day\n",
    "    return group.sum() / registered_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import calendar\n",
    "# Compute mean per month per country. \n",
    "results = df.groupby(lambda row: (row[0], row[1].month))\\\n",
    "            .agg(month_average)\n",
    "\n",
    "# Give computation result a nice index\n",
    "results.index = pd.MultiIndex.from_tuples(results.index, names=['Country', 'Month'])\\\n",
    "                  .map(lambda i: (i[0], calendar.month_name[i[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. RNA Sequences\n",
    "\n",
    "In the `DATA_FOLDER/microbiome` subdirectory, there are 9 spreadsheets of microbiome data that was acquired from high-throughput RNA sequencing procedures, along with a 10<sup>th</sup> file that describes the content of each. \n",
    "\n",
    "Use pandas to import the first 9 spreadsheets into a single `DataFrame`.\n",
    "Then, add the metadata information from the 10<sup>th</sup> spreadsheet as columns in the combined `DataFrame`.\n",
    "Make sure that the final `DataFrame` has a unique index and all the `NaN` values have been replaced by the tag `unknown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Class War in Titanic\n",
    "\n",
    "Use pandas to import the data file `Data/titanic.xls`. It contains data on all the passengers that travelled on the Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(filename=DATA_FOLDER+'/titanic.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the following questions state clearly your assumptions and discuss your findings:\n",
    "1. Describe the *type* and the *value range* of each attribute. Indicate and transform the attributes that can be `Categorical`. \n",
    "2. Plot histograms for the *travel class*, *embarkation port*, *sex* and *age* attributes. For the latter one, use *discrete decade intervals*. \n",
    "3. Calculate the proportion of passengers by *cabin floor*. Present your results in a *pie chart*.\n",
    "4. For each *travel class*, calculate the proportion of the passengers that survived. Present your results in *pie charts*.\n",
    "5. Calculate the proportion of the passengers that survived by *travel class* and *sex*. Present your results in *a single histogram*.\n",
    "6. Create 2 equally populated *age categories* and calculate survival proportions by *age category*, *travel class* and *sex*. Present your results in a `DataFrame` with unique index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
